{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwZknXh9U/zgkz+xcr60k8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jingz666/NeuF/blob/main/NeuS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "VY71-SwfpGGA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encoding embedding. Code was taken from https://github.com/bmild/nerf."
      ],
      "metadata": {
        "id": "f_QSOEJSo2b5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9WJwwA6fo1G1"
      },
      "outputs": [],
      "source": [
        "class Embedder:\n",
        "  def __init__(self, **kwargs):\n",
        "    self.kwargs = kwargs\n",
        "    self.create_embedding_fn()\n",
        "\n",
        "  def create_embedding_fn(self):\n",
        "    embed_fns = []\n",
        "    d = self.kwargs['input_dims']\n",
        "    out_dim = 0\n",
        "    if self.kwargs['include_input']:\n",
        "      embed_fns.append(lambda x:x)\n",
        "      out_dim += d\n",
        "    \n",
        "    max_freq = self.kwargs['max_freq_log2']\n",
        "    N_freqs = self.kwargs['num_freqs']\n",
        "\n",
        "    if self.kwargs['log_sampling']:\n",
        "      freq_bands = 2. ** torch.linspace(0., max_freq, N_freqs) # tensor([  1.,   2.,   4.,   8.,  16.,  32.,  64., 128., 256., 512.])\n",
        "    else:\n",
        "      freq_bands = torch.linspace(2.**0, 2.**max_freq, N_freqs) # tensor([  1.0000,  57.7778, 114.5556, 171.3333, 228.1111, 284.8889, 341.6667, 398.4445, 455.2222, 512.0000])\n",
        "\n",
        "    for freq in freq_bands:\n",
        "      for p_fn in self.kwargs['periodic_fns']:\n",
        "        embed_fns.append(lambda x, p_fn=p_fn, freq=freq: p_fn(x * freq))\n",
        "        out_dim += d\n",
        "\n",
        "    self.embed_fns = embed_fns\n",
        "    self.out_dim = out_dim\n",
        "\n",
        "    def embed(self, inputs):\n",
        "      return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
        "\n",
        "def get_embedder(multires, input_dims=3):\n",
        "  embed_kwargs = {\n",
        "      'include_input': True,\n",
        "      'input_dims': input_dims,\n",
        "      'max_freq_log2': multires-1,\n",
        "      'num_freqs': multires,\n",
        "      'log_sampling': True,\n",
        "      'periodic_fns': [torch.sin, torch.cos],\n",
        "  }\n",
        "\n",
        "  embedder_obj = Embedder(**embed_kwargs)\n",
        "  def embed(x, eo=embedder_obj): return eo.embed(x)\n",
        "  return embed, embedder_obj.out_dim "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This implementation is borrowed from nerf-pytorch: https://github.com/yenchenlin/nerf-pytorch"
      ],
      "metadata": {
        "id": "8tUJ_Jvgj6Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Wlxo0GfNoICC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeRF(nn.Module):\n",
        "  def __init__(self, D=8, W=256, d_in=3, d_in_view=3, multires=0, multires_view=0, output_ch=4, skips=[4], use_viewdirs=False):\n",
        "    super(NeRF, self).__init__()\n",
        "    self.D = D\n",
        "    self.W = W\n",
        "    self.d_in = d_in\n",
        "    self.d_in_view = d_in_view\n",
        "    self.input_ch = 3\n",
        "    self.input_ch_view = 3\n",
        "    self.embed_fn = None\n",
        "    self.embed_fn_view = None\n",
        "\n",
        "    if multires > 0:\n",
        "      embed_fn, input_ch = get_embedder(multires, input_dims=d_in)\n",
        "      self.embed_fn = embed_fn\n",
        "      self.input_ch = input_ch\n",
        "\n",
        "    if multires_view > 0:\n",
        "      embed_fn_view, input_ch_view = get_embedder(multires_view, input_dims=d_in_view)\n",
        "      self.embed_fn_view = embed_fn_view\n",
        "      self.input_ch_view = input_ch_view\n",
        "\n",
        "    self.skips = skips\n",
        "    self.use_viewdirs = use_viewdirs\n",
        "\n",
        "#     ModuleList(\n",
        "#   (0): Linear(in_features=36, out_features=256, bias=True)\n",
        "#   (1): Linear(in_features=256, out_features=256, bias=True)\n",
        "#   (2): Linear(in_features=256, out_features=256, bias=True)\n",
        "#   (3): Linear(in_features=256, out_features=256, bias=True)\n",
        "#   (4): Linear(in_features=256, out_features=256, bias=True)\n",
        "#   (5): Linear(in_features=292, out_features=256, bias=True)\n",
        "#   (6): Linear(in_features=256, out_features=256, bias=True)\n",
        "#   (7): Linear(in_features=256, out_features=256, bias=True)\n",
        "# )\n",
        "    self.pts_linears = nn.ModuleList(\n",
        "        [nn.Linear(self.input_ch, W)]+\n",
        "        [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + self.input_ch, W) for i in range(D - 1)] # i=0,1,...,7\n",
        "    )\n",
        "\n",
        "    self.views_linears = nn.ModuleList([nn.Linear(self.input_ch_view + W, W // 2)])\n",
        "\n",
        "    if use_viewdirs:\n",
        "      self.feature_linear = nn.Linear(W, W)\n",
        "      self.alpha_linear = nn.Linear(W, 1)\n",
        "      self.rgb_linear = nn.Linear(W // 2, 3)\n",
        "    else:\n",
        "      self.output_linear = nn.Linear(W, output_ch)\n",
        "  \n",
        "  def forward(self, input_pts, input_views):\n",
        "    if self.embed_fn is not None:\n",
        "      input_pts = self.embed_fn(input_pts)\n",
        "    if self.embed_fn_view is not None:\n",
        "      input_views = self.embed_fn_view(input_views)\n",
        "    \n",
        "    h = input_pts\n",
        "    for i, l in enumerate(self.pts_linears):\n",
        "      h = self.pts_linears[i](h)\n",
        "      h = F.relu(h)\n",
        "      if i in self.skips:\n",
        "        h = torch.cat([input_pts, h], -1)\n",
        "    \n",
        "    if self.use_viewdirs:\n",
        "      alpha = self.alpha_linear(h)\n",
        "      feature = self.feature_linear(h)\n",
        "      h = torch.cat([feature, input_views], -1)\n",
        "\n",
        "      for i,l in enumerate(self.views_linears):\n",
        "        h = self.views_linears[i](h)\n",
        "        h = F.relu(h)\n",
        "\n",
        "      rgb = self.rgb_linear(h)\n",
        "      return alpha, rgb\n",
        "    else:\n",
        "      assert False\n",
        "\n",
        "# This implementation is borrowed from IDR: https://github.com/lioryariv/idr\n",
        "class"
      ],
      "metadata": {
        "id": "Uza5H4ZntLtE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.ModuleList([nn.Linear(36 + 256, 256 // 2)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJouv1nfuNaf",
        "outputId": "3396d6f6-cc41-4f6c-e453-d160c5a739f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): Linear(in_features=292, out_features=128, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(i) if i not in [4] else nn.Linear(256 + 36, 256) for i in range(8 - 1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "TBOrIrndqAe0",
        "outputId": "b6954b5d-2d66-4496-e8f2-3a492b9deb3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-bf6c44fe9270>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(i) if i not in [4] else nn.Linear(256 + 36, 256) for i in range(8 - 1)]\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}